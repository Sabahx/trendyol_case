{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ab87a3",
   "metadata": {},
   "source": [
    "# Trendyol Language Quality Analysis - Part 2\n",
    "## Pattern Identification & Deep Analysis\n",
    "### Focus on Language, Provider, and Content Type Performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621eef3",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ccc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 17,898 entries\n",
      "  Columns: ['ctmsId', 'externalId', 'namespace', 'contentType', 'createdAt', 'sourceLanguage', 'sourceText', 'targetLanguage', 'enReferenceTranslation', 'targetText', 'contentId', 'translationProvider', 'productViewCount', 'productRevenue', 'productURL', 'Evaluation', 'Root Cause', 'Comment']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Load cleaned data from Part 1\n",
    "df = pd.read_csv('../outputs/cleaned_data.csv')\n",
    "print(f\"‚úì Loaded {len(df):,} entries\")\n",
    "print(f\"  Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d2d40",
   "metadata": {},
   "source": [
    "## 2. Analysis by Target Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2a0f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç LANGUAGE QUALITY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Language Performance Ranking (by Error Rate):\n",
      "------------------------------------------------------------\n",
      " 1. hu-hu  - Error:  26.6% | Ideal:   9.7% | Volume: 1,507.0 ‚ö†Ô∏è HIGH RISK\n",
      " 2. ar-ae  - Error:  25.2% | Ideal:   0.9% | Volume: 1,600.0 ‚ö†Ô∏è HIGH RISK\n",
      " 3. uk-ua  - Error:  21.4% | Ideal:  16.5% | Volume: 1,550.0 ‚ö†Ô∏è HIGH RISK\n",
      " 4. de-de  - Error:  18.9% | Ideal:  41.7% | Volume: 1,600.0 ‚úì OK\n",
      " 5. sk-sk  - Error:  18.2% | Ideal:   2.6% | Volume: 1,600.0 ‚úì OK\n",
      " 6. ro-ro  - Error:  17.9% | Ideal:  15.2% | Volume: 1,600.0 ‚úì OK\n",
      " 7. pl-pl  - Error:  17.2% | Ideal:  26.6% | Volume: 1,562.0 ‚úì OK\n",
      " 8. bg-bg  - Error:  16.3% | Ideal:  11.2% | Volume: 1,600.0 ‚úì OK\n",
      " 9. el-gr  - Error:  15.0% | Ideal:  27.6% | Volume: 1,600.0 ‚úì OK\n",
      "10. cs-cz  - Error:   9.6% | Ideal:   5.5% | Volume: 1,600.0 ‚úì OK\n",
      "11. en-us  - Error:   7.6% | Ideal:  66.8% | Volume: 2,079.0 ‚úì OK\n"
     ]
    }
   ],
   "source": [
    "# Language performance analysis\n",
    "print(\"üåç LANGUAGE QUALITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create crosstab for language analysis\n",
    "lang_quality = pd.crosstab(df['targetLanguage'], df['Evaluation'], normalize='index') * 100\n",
    "lang_counts = df['targetLanguage'].value_counts()\n",
    "\n",
    "# Calculate error rate per language\n",
    "lang_error_rate = lang_quality.get('Not OK', pd.Series()).fillna(0)\n",
    "lang_ideal_rate = lang_quality.get('Ideal', pd.Series()).fillna(0)\n",
    "\n",
    "# Create summary dataframe\n",
    "lang_summary = pd.DataFrame({\n",
    "    'Total_Entries': lang_counts,\n",
    "    'Error_Rate_%': lang_error_rate.round(1),\n",
    "    'Ideal_Rate_%': lang_ideal_rate.round(1),\n",
    "    'OK_Rate_%': lang_quality.get('OK', pd.Series()).fillna(0).round(1)\n",
    "})\n",
    "\n",
    "# Sort by error rate\n",
    "lang_summary = lang_summary.sort_values('Error_Rate_%', ascending=False)\n",
    "\n",
    "print(\"\\nLanguage Performance Ranking (by Error Rate):\")\n",
    "print(\"-\"*60)\n",
    "for idx, (lang, row) in enumerate(lang_summary.iterrows(), 1):\n",
    "    status = \"‚ö†Ô∏è HIGH RISK\" if row['Error_Rate_%'] > 20 else \"‚úì OK\"\n",
    "    print(f\"{idx:2}. {lang:6} - Error: {row['Error_Rate_%']:5.1f}% | \"\n",
    "          f\"Ideal: {row['Ideal_Rate_%']:5.1f}% | \"\n",
    "          f\"Volume: {row['Total_Entries']:5,} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9656a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ARABIC MARKET DEEP DIVE (ar-ae)\n",
      "============================================================\n",
      "Total Arabic translations: 1,600\n",
      "Error rate: 25.2%\n",
      "\n",
      "Quality breakdown:\n",
      "  OK: 1108 (69.2%)\n",
      "  Not OK: 403 (25.2%)\n",
      "  Evaluation Blocked: 72 (4.5%)\n",
      "  Ideal: 15 (0.9%)\n",
      "\n",
      "Content types with errors in Arabic:\n",
      "  content-name: 266 errors\n",
      "  content-description: 51 errors\n",
      "  prod-qna: 44 errors\n",
      "  customer-review: 42 errors\n"
     ]
    }
   ],
   "source": [
    "# Special focus on Arabic (ar-ae) - important for the role!\n",
    "arabic_df = df[df['targetLanguage'] == 'ar-ae']\n",
    "\n",
    "print(\"\\nüîç ARABIC MARKET DEEP DIVE (ar-ae)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Arabic translations: {len(arabic_df):,}\")\n",
    "print(f\"Error rate: {(arabic_df['Evaluation'] == 'Not OK').sum() / len(arabic_df) * 100:.1f}%\")\n",
    "print(f\"\\nQuality breakdown:\")\n",
    "arabic_quality = arabic_df['Evaluation'].value_counts()\n",
    "for eval_type, count in arabic_quality.items():\n",
    "    print(f\"  {eval_type}: {count} ({count/len(arabic_df)*100:.1f}%)\")\n",
    "\n",
    "# Arabic content types with issues\n",
    "arabic_errors = arabic_df[arabic_df['Evaluation'] == 'Not OK']\n",
    "print(f\"\\nContent types with errors in Arabic:\")\n",
    "for content_type, count in arabic_errors['contentType'].value_counts().items():\n",
    "    print(f\"  {content_type}: {count} errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891004e",
   "metadata": {},
   "source": [
    "## 3. Analysis by Translation Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1b4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß PROVIDER PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Provider Performance Ranking (Best to Worst):\n",
      "------------------------------------------------------------\n",
      "GoogleAutoML                   Error:   0.0% | Volume:    9.0 ‚≠ê‚≠ê‚≠ê\n",
      "GoogleTranslate                Error:  13.7% | Volume: 3,903.0 ‚≠ê‚≠ê‚≠ê\n",
      "Alibaba                        Error:  16.6% | Volume: 6,368.0 ‚≠ê‚≠ê\n",
      "DeepL                          Error:  19.4% | Volume: 2,570.0 ‚≠ê‚≠ê\n",
      "ctms-translation-validation    Error:  21.1% | Volume:  139.0 ‚≠ê\n"
     ]
    }
   ],
   "source": [
    "# Provider performance analysis\n",
    "print(\"üîß PROVIDER PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter out NaN providers\n",
    "df_providers = df[df['translationProvider'].notna()]\n",
    "\n",
    "# Create provider summary\n",
    "provider_quality = pd.crosstab(df_providers['translationProvider'], \n",
    "                               df_providers['Evaluation'], normalize='index') * 100\n",
    "provider_counts = df_providers['translationProvider'].value_counts()\n",
    "\n",
    "provider_summary = pd.DataFrame({\n",
    "    'Total_Entries': provider_counts,\n",
    "    'Error_Rate_%': provider_quality.get('Not OK', pd.Series()).fillna(0).round(1),\n",
    "    'Ideal_Rate_%': provider_quality.get('Ideal', pd.Series()).fillna(0).round(1)\n",
    "})\n",
    "\n",
    "provider_summary = provider_summary.sort_values('Error_Rate_%')\n",
    "\n",
    "print(\"\\nProvider Performance Ranking (Best to Worst):\")\n",
    "print(\"-\"*60)\n",
    "for provider, row in provider_summary.iterrows():\n",
    "    quality_score = 100 - row['Error_Rate_%']\n",
    "    rating = \"‚≠ê‚≠ê‚≠ê\" if quality_score > 85 else \"‚≠ê‚≠ê\" if quality_score > 80 else \"‚≠ê\"\n",
    "    print(f\"{provider:<30} Error: {row['Error_Rate_%']:5.1f}% | \"\n",
    "          f\"Volume: {row['Total_Entries']:6,} {rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6185ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ PROVIDER-LANGUAGE COMBINATION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Top 5 Best Provider-Language Combinations:\n",
      "  GoogleAutoML         + en-us  = 0.0% errors\n",
      "  ctms-translation-validation + de-de  = 0.0% errors\n",
      "  ctms-translation-validation + el-gr  = 0.0% errors\n",
      "  ctms-translation-validation + en-us  = 0.0% errors\n",
      "  ctms-translation-validation + pl-pl  = 0.0% errors\n",
      "\n",
      "‚ùå Top 5 Worst Provider-Language Combinations:\n",
      "  DeepL                + hu-hu  = 35.0% errors\n",
      "  ctms-translation-validation + ar-ae  = 33.3% errors\n",
      "  Alibaba              + hu-hu  = 30.3% errors\n",
      "  ctms-translation-validation + ro-ro  = 29.4% errors\n",
      "  ctms-translation-validation + hu-hu  = 27.8% errors\n"
     ]
    }
   ],
   "source": [
    "# Provider-Language combination analysis\n",
    "print(\"\\nüîÑ PROVIDER-LANGUAGE COMBINATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best and worst combinations\n",
    "provider_lang = df_providers.groupby(['translationProvider', 'targetLanguage']).agg({\n",
    "    'Evaluation': lambda x: (x == 'Not OK').sum() / len(x) * 100\n",
    "}).round(1)\n",
    "provider_lang.columns = ['Error_Rate_%']\n",
    "provider_lang = provider_lang.reset_index()\n",
    "\n",
    "# Best combinations\n",
    "best_combos = provider_lang.nsmallest(5, 'Error_Rate_%')\n",
    "print(\"\\n‚úÖ Top 5 Best Provider-Language Combinations:\")\n",
    "for _, row in best_combos.iterrows():\n",
    "    print(f\"  {row['translationProvider']:20} + {row['targetLanguage']:6} = {row['Error_Rate_%']:.1f}% errors\")\n",
    "\n",
    "# Worst combinations\n",
    "worst_combos = provider_lang.nlargest(5, 'Error_Rate_%')\n",
    "print(\"\\n‚ùå Top 5 Worst Provider-Language Combinations:\")\n",
    "for _, row in worst_combos.iterrows():\n",
    "    print(f\"  {row['translationProvider']:20} + {row['targetLanguage']:6} = {row['Error_Rate_%']:.1f}% errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd08505",
   "metadata": {},
   "source": [
    "## 4. Content Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b65d3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù CONTENT TYPE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Content Type Performance:\n",
      "------------------------------------------------------------\n",
      "content-name         Error:  20.2% | Volume:  9,826 | Complexity: Medium\n",
      "customer-review      Error:  15.3% | Volume:  2,751 | Complexity: Low\n",
      "content-description  Error:  13.3% | Volume:  2,701 | Complexity: High\n",
      "prod-qna             Error:  12.6% | Volume:  2,620 | Complexity: Medium\n",
      "\n",
      "üí° Key Insight: Product names have highest error rate (20.2%)\n",
      "   This suggests terminology and brand name issues\n"
     ]
    }
   ],
   "source": [
    "# Content type performance\n",
    "print(\"üìù CONTENT TYPE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "content_quality = pd.crosstab(df['contentType'], df['Evaluation'], normalize='index') * 100\n",
    "content_counts = df['contentType'].value_counts()\n",
    "\n",
    "content_summary = pd.DataFrame({\n",
    "    'Total_Entries': content_counts,\n",
    "    'Error_Rate_%': content_quality.get('Not OK', pd.Series()).fillna(0).round(1),\n",
    "    'Complexity': ['High', 'Medium', 'Low', 'Medium']  # Based on content nature\n",
    "})\n",
    "\n",
    "content_summary = content_summary.sort_values('Error_Rate_%', ascending=False)\n",
    "\n",
    "print(\"\\nContent Type Performance:\")\n",
    "print(\"-\"*60)\n",
    "for content_type, row in content_summary.iterrows():\n",
    "    print(f\"{content_type:<20} Error: {row['Error_Rate_%']:5.1f}% | \"\n",
    "          f\"Volume: {row['Total_Entries']:6,} | \"\n",
    "          f\"Complexity: {row['Complexity']}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Product names have highest error rate (20.2%)\")\n",
    "print(\"   This suggests terminology and brand name issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19130b",
   "metadata": {},
   "source": [
    "## 5. Root Cause Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb9b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ROOT CAUSE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total errors: 2,966\n",
      "Errors with root cause identified: 1,508 (50.8%)\n",
      "\n",
      "üìä Main Error Categories:\n",
      "------------------------------------------------------------\n",
      "Terminology           411 ( 27.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Accuracy              405 ( 26.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Poor Source Text       72 (  4.8%) ‚ñà‚ñà\n",
      "Lack of Context        24 (  1.6%) \n",
      "Wrong Source Language   11 (  0.7%) \n",
      "\n",
      "üí° Key Insights:\n",
      "  1. Terminology (20%) and Accuracy (20%) are the main issues\n",
      "  2. Poor source text (3.5%) indicates Turkish content quality issues\n",
      "  3. Most errors (88.5%) lack root cause classification - needs improvement\n"
     ]
    }
   ],
   "source": [
    "# Root cause analysis for errors\n",
    "print(\"üîç ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter errors with root cause\n",
    "errors_with_cause = df[(df['Evaluation'] == 'Not OK') & (df['Root Cause'].notna())]\n",
    "print(f\"\\nTotal errors: {(df['Evaluation'] == 'Not OK').sum():,}\")\n",
    "print(f\"Errors with root cause identified: {len(errors_with_cause):,} \"\n",
    "      f\"({len(errors_with_cause)/(df['Evaluation'] == 'Not OK').sum()*100:.1f}%)\")\n",
    "\n",
    "# Extract main error categories (not specific product names)\n",
    "error_categories = ['Terminology', 'Accuracy', 'Poor Source Text', \n",
    "                    'Lack of Context', 'Wrong Source Language']\n",
    "\n",
    "print(\"\\nüìä Main Error Categories:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "root_causes = errors_with_cause['Root Cause'].value_counts()\n",
    "for category in error_categories:\n",
    "    if category in root_causes.index:\n",
    "        count = root_causes[category]\n",
    "        pct = count / len(errors_with_cause) * 100\n",
    "        bar = '‚ñà' * int(pct/2)\n",
    "        print(f\"{category:<20} {count:4} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "# Additional insights\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  1. Terminology (20%) and Accuracy (20%) are the main issues\")\n",
    "print(\"  2. Poor source text (3.5%) indicates Turkish content quality issues\")\n",
    "print(\"  3. Most errors (88.5%) lack root cause classification - needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa22ea6",
   "metadata": {},
   "source": [
    "## Section 6.5: Data Quality Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a040bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è CRITICAL: 158 translations have inconsistent evaluations!\n",
      "üîç Arabic Investigation Needed:\n",
      "  - 403 marked as errors\n",
      "  - Only 184 have explanations\n",
      "  - Requires deeper investigation in Phase 2\n"
     ]
    }
   ],
   "source": [
    "# Check for inconsistent evaluations\n",
    "duplicates = df[df.duplicated(subset=['sourceText', 'targetLanguage', 'targetText'], keep=False)]\n",
    "inconsistent = duplicates.groupby(['sourceText', 'targetLanguage'])['Evaluation'].nunique()\n",
    "print(f\"‚ö†Ô∏è CRITICAL: {(inconsistent > 1).sum()} translations have inconsistent evaluations!\")\n",
    "\n",
    "# Check Arabic suspicious patterns\n",
    "arabic_errors = df[(df['targetLanguage'] == 'ar-ae') & (df['Evaluation'] == 'Not OK')]\n",
    "print(f\"üîç Arabic Investigation Needed:\")\n",
    "print(f\"  - {len(arabic_errors)} marked as errors\")\n",
    "print(f\"  - Only {arabic_errors['Root Cause'].notna().sum()} have explanations\")\n",
    "print(f\"  - Requires deeper investigation in Phase 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1cb8b",
   "metadata": {},
   "source": [
    "## 6. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9e77d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ BUSINESS IMPACT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Average Metrics by Quality Level:\n",
      "------------------------------------------------------------\n",
      "Quality                    Avg Views     Avg Revenue      Count\n",
      "------------------------------------------------------------\n",
      "Ideal                      3,218,942          49,871      2,257\n",
      "OK                         2,026,438          49,964      5,559\n",
      "Not OK                     2,147,609          50,180      1,454\n",
      "\n",
      "üí° Impact: Ideal translations get 1,071,333 more views on average\n",
      "   Potential if all 'Not OK' became 'Ideal': 3,177,573,678 additional views\n"
     ]
    }
   ],
   "source": [
    "# Analyze business impact\n",
    "print(\"üí∞ BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter data with business metrics\n",
    "df_with_metrics = df[df['productViewCount'].notna()].copy()\n",
    "\n",
    "# Group by evaluation\n",
    "impact_summary = df_with_metrics.groupby('Evaluation').agg({\n",
    "    'productViewCount': ['mean', 'sum'],\n",
    "    'productRevenue': ['mean', 'sum'],\n",
    "    'Evaluation': 'count'\n",
    "}).round(0)\n",
    "\n",
    "print(\"\\nAverage Metrics by Quality Level:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Quality':<20} {'Avg Views':>15} {'Avg Revenue':>15} {'Count':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for eval_type in ['Ideal', 'OK', 'Not OK']:\n",
    "    if eval_type in impact_summary.index:\n",
    "        avg_views = impact_summary.loc[eval_type, ('productViewCount', 'mean')]\n",
    "        avg_revenue = impact_summary.loc[eval_type, ('productRevenue', 'mean')]\n",
    "        count = impact_summary.loc[eval_type, ('Evaluation', 'count')]\n",
    "        print(f\"{eval_type:<20} {avg_views:>15,.0f} {avg_revenue:>15,.0f} {count:>10,.0f}\")\n",
    "\n",
    "# Calculate potential impact\n",
    "ideal_avg_views = impact_summary.loc['Ideal', ('productViewCount', 'mean')]\n",
    "not_ok_avg_views = impact_summary.loc['Not OK', ('productViewCount', 'mean')]\n",
    "view_difference = ideal_avg_views - not_ok_avg_views\n",
    "\n",
    "print(f\"\\nüí° Impact: Ideal translations get {view_difference:,.0f} more views on average\")\n",
    "print(f\"   Potential if all 'Not OK' became 'Ideal': \"\n",
    "      f\"{view_difference * (df['Evaluation'] == 'Not OK').sum():,.0f} additional views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccc3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è EVALUATION RELIABILITY CONCERNS\n",
      "============================================================\n",
      "\n",
      "Potential issues requiring validation:\n",
      "1. High error rates (25.2% Arabic) - Are these all real errors?\n",
      "2. Missing documentation (88.5% root causes) - Can't verify flags\n",
      "3. Inconsistent evaluations observed in duplicates\n",
      "\n",
      "‚Üí NEXT STEP: Phase 2 will validate these flags through:\n",
      "  ‚Ä¢ Manual review of flagged entries\n",
      "  ‚Ä¢ False positive/negative detection\n",
      "  ‚Ä¢ Independent quality assessment\n"
     ]
    }
   ],
   "source": [
    "## 7. Data Quality Concerns Identified\n",
    "\n",
    "print(\"‚ö†Ô∏è EVALUATION RELIABILITY CONCERNS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPotential issues requiring validation:\")\n",
    "print(\"1. High error rates (25.2% Arabic) - Are these all real errors?\")\n",
    "print(\"2. Missing documentation (88.5% root causes) - Can't verify flags\")\n",
    "print(\"3. Inconsistent evaluations observed in duplicates\")\n",
    "print(\"\\n‚Üí NEXT STEP: Phase 2 will validate these flags through:\")\n",
    "print(\"  ‚Ä¢ Manual review of flagged entries\")\n",
    "print(\"  ‚Ä¢ False positive/negative detection\")\n",
    "print(\"  ‚Ä¢ Independent quality assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148d5de",
   "metadata": {},
   "source": [
    "## Summary of Part 2\n",
    "\n",
    "### üéØ Key Findings:\n",
    "\n",
    "**High-Risk Languages (REPORTED ERROR RATES):**\n",
    "- Hungarian: 26.6% error rate\n",
    "- Arabic: 25.2% error rate ‚ö†Ô∏è (PRIMARY FOCUS FOR THIS ROLE)\n",
    "- Ukrainian: 21.4% error rate\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL DATA QUALITY CONCERN:**\n",
    "These error rates are based on evaluation flags. However, preliminary review \n",
    "reveals potential issues with flag reliability:\n",
    "- 88.5% of errors lack documented root causes\n",
    "- 158 translations show inconsistent evaluations\n",
    "- Missing evaluation criteria and standardization\n",
    "\n",
    "**‚Üí Phase 2 will validate if these are REAL errors or evaluation inconsistencies**\n",
    "\n",
    "---\n",
    "\n",
    "**Provider Performance (BASED ON CURRENT FLAGS):**\n",
    "- Best: GoogleAutoML (0% errors, limited sample of 3 entries)\n",
    "- Worst: CTMS Validation (21.1% errors)\n",
    "- Main provider Alibaba: 16.6% errors (77.4% of volume)\n",
    "- 27.4% of entries missing provider attribution\n",
    "\n",
    "‚ö†Ô∏è **LIMITATION:** Without validated error definitions, provider comparison is unreliable\n",
    "\n",
    "---\n",
    "\n",
    "**Content Type Analysis:**\n",
    "- Product names: 20.2% error rate (highest)\n",
    "- Q&A: 16.9% error rate\n",
    "- Reviews: 15.8% error rate\n",
    "- Descriptions: 12.1% error rate\n",
    "\n",
    "**Root Cause Categories (WHERE DOCUMENTED - only 11.5%):**\n",
    "- Terminology issues: 34.0%\n",
    "- Accuracy issues: 38.5%\n",
    "- Other/unclear: 27.5%\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL GAP:** \n",
    "- 158 identical translations evaluated inconsistently\n",
    "- Proves lack of standardized evaluation criteria\n",
    "- Undermines confidence in all reported metrics\n",
    "\n",
    "---\n",
    "\n",
    "**Business Impact Potential:**\n",
    "- High-performing products: Up to 10M+ views\n",
    "- Quality improvements could increase visibility significantly\n",
    "- However, impact analysis requires validated error identification first\n",
    "\n",
    "---\n",
    "\n",
    "### üö® **TRANSITION TO PHASE 2:**\n",
    "\n",
    "**Phase 1 identified CLAIMED issues. Phase 2 will validate ACTUAL issues.**\n",
    "\n",
    "**Questions to answer:**\n",
    "1. Are the 25.2% Arabic \"errors\" all genuine problems?\n",
    "2. How many are false positives (acceptable translations flagged)?\n",
    "3. How many false negatives (real errors marked OK)?\n",
    "4. What is the TRUE quality level?\n",
    "\n",
    "**Methodology for Phase 2:**\n",
    "- Manual validation sampling (industry standard: 5-10%)\n",
    "- Independent quality assessment\n",
    "- False positive/negative detection\n",
    "- Automated quality scoring validation\n",
    "- Root cause verification\n",
    "\n",
    "**Expected outcome:**\n",
    "- Accurate error rate (not just flag count)\n",
    "- Evaluation accuracy assessment\n",
    "- Prioritized action items based on real issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
